{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb1dd5a-aea3-4614-96ff-6354178af179",
   "metadata": {},
   "source": [
    "# Semantic Q&A System on Research Documents\n",
    "\n",
    "## Introduction:\n",
    "This project implements a Retrieval-Augmented Generation (RAG) system that combines semantic search with generative capabilities to answer queries based on a research paper. By leveraging FAISS for dense vector retrieval and TinyLLaMA for lightweight question generation, the system offers efficient and context-aware responses to user queries. The document used is a scientific research paper (RAGPAPER.pdf), and the system supports conversational memory, allowing follow-up questions to be contextualized appropriately.\n",
    "AIM To build a lightweight and efficient Retrieval-Augmented Generation (RAG) pipeline that utilizes document embeddings and a small LLM to semantically understand, retrieve, and respond to questions about a given PDF document.\n",
    "\n",
    "## Objectives:\n",
    "- Load and parse research PDFs using LangChain document loaders.\n",
    "- Split documents into manageable chunks for semantic search.\n",
    "- Generate embeddings using HuggingFace Embedding models.\n",
    "- Store and retrieve document vectors using FAISS vector store.\n",
    "- Integrate Ollama to run the TinyLLaMA language model locally.\n",
    "- Build a ConversationalRetrievalChain for multi-turn Q&A.\n",
    "- Rephrase follow-up questions into standalone prompts using prompt engineering.\n",
    "- Provide accurate, contextually grounded answers to user queries.\n",
    "\n",
    "## Model & Configurations:\n",
    "- LLM: TinyLLaMA – a compact large language model suitable for low-resource inference.\n",
    "- Embedding Model: HuggingFaceEmbeddings (Default is sentence-transformers/all-MiniLM-L6-v2)\n",
    "- Vector Store: FAISS – Facebook AI Similarity Search, used for fast retrieval of top relevant chunks.\n",
    "- Prompt Engineering: Custom prompt using PromptTemplate to rephrase follow-up questions to standalone ones.\n",
    "\n",
    "## TinyLLaMA Parameters:\n",
    "- Model Name: TinyLLaMA-1.1B\n",
    "- Number of Parameters: 1.1 Billion\n",
    "- Architecture: Decoder-only Transformer (similar to LLaMA)\n",
    "- Trained on: The RedPajama dataset and additional curated datasets\n",
    "- Tokenizer: LLaMA-compatible tokenizer with 32k vocab size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9947b277-833c-4750-8110-114835690d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfd139cc-fcca-42e6-90a8-bfbf786a0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk PDF\n",
    "pdf_loader = PyPDFLoader('RAGPAPER.pdf')\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40511b02-a647-40dc-a860-5a743648ab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasir\\AppData\\Local\\Temp\\ipykernel_11976\\1535790851.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_model = HuggingFaceEmbeddings()\n",
      "C:\\Users\\Nasir\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Embedding and Vector Store\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents=chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "314939ae-e37e-4d48-8466-436d77116459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model\n",
    "llm_model = Ollama(model='tinyllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0446658a-84ed-4f0d-bb73-8300b51f1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template for standalone question\n",
    "question_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Given the following conversation and follow-up question, rephrase the follow-up question to a standalone question.\n",
    "Chat History: {chat_history}\n",
    "Follow-up Input: {question}\n",
    "Standalone Question:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "246da91b-ee71-496f-9f22-c4c0fc380375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational Retrieval Chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_model,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    condense_question_prompt=question_prompt,\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e885b588-22db-4cf5-8d41-28925ef38621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat history and sample query\n",
    "chat_history = []\n",
    "query = \"What are different Indexing Optimization methods used in this paper?\"\n",
    "results = qa_chain({'question': query, 'chat_history': chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ae7422ee-fe57-49f6-a2bb-a07a1cd7396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Different Indexing Optimization methods used in this paper are Chunking Strategy, Enhancinig Data Granularity, Adding Metadata, Alignmnet Optimization, and Post-Retrieval Process. These methods aim to enhance the quality of the content being indexed by strategically optimizing index structures, optimizing query structures, aligning queries with retrieved information, re-ranking retrieved information to relate most relevant documents to edges of prompts, feeding metadata from original documents directly into LLMs for direct relevance retrieval, and establishing hierarchical structures for documents. These methods aid in the swift traversal of data and assist RAG systems in determining which documents are pertinent to a user's original question.\n"
     ]
    }
   ],
   "source": [
    "# Output answer\n",
    "print(\"Answer:\", results['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a3ce4-b1d1-4fce-bddb-359b20a0e596",
   "metadata": {},
   "source": [
    "### Limitations \n",
    "- Model Capacity: TinyLLaMA (1.1B params) may struggle with complex or highly technical queries compared to larger LLMs.\n",
    "- Context Window: Limited token size can cause incomplete understanding of lengthy documents.\n",
    "- System Dependency: Performance depends on local CPU/GPU; low-end systems may face slowdowns.\n",
    "- General-Purpose Model: Not fine-tuned for academic/research texts, affecting precision.\n",
    "- Static Data Scope: Answers are limited to the uploaded PDF content only.\n",
    "\n",
    "### Future Work\n",
    "- Model Upgrade: Integrate larger or fine-tuned models (e.g., LLaMA-2, GPT-3.5) for better understanding and accuracy.\n",
    "- Domain Adaptation: Fine-tune the model on academic corpora to improve performance on technical queries\n",
    "- Multi-Document Support: Expand system to handle and cross-reference multiple research papers.\n",
    "- Web-Based UI: Develop a user-friendly web interface using Streamlit or Gradio for broader usability.\n",
    "- Hybrid Retrieval: Combine vector search with keyword-based methods for more robust question answering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08dd3d-2da5-4c22-8a1a-35c338746453",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project demonstrates the power of combining retrieval-based search with generative models, even with lightweight LLMs like TinyLLaMA. It bridges the gap between static document search and intelligent conversational interfaces. The use of FAISS ensures fast, scalable vector search, while LangChain’s tools enable modular and extendable pipeline development. This system can be adapted for document Q&A, legal or research assistants, and educational tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
